{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "# Torchvision\n",
    "import torchvision \n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import ToTensor # Convert a PIL Image or ndarray to tensor and scale the values accordingly.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Checking versions\n",
    "\n",
    "print(F\"PyTorch versrion : {torch.__version__}\")\n",
    "print(F\"TorchVision version : {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None, \n",
    ")\n",
    "\n",
    "# Testing Data \n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_data[0]\n",
    "print(F\"image: {image} \\n \\n label : {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and Output shapes of data \n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9010362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Shape \n",
    "\n",
    "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names  = train_data.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcefe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(class_names[label]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f88a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GreyScale \n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot more images\n",
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "rows, cols = 4, 4\n",
    "for i in range(1, rows * cols + 1):\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crearting DataLoader \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Batch-size Hyperparameter \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# turining data into iterables ( Batches )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data, # dataset to be turned into an iterable \n",
    "    batch_size=BATCH_SIZE, # How many samples per batch \n",
    "    shuffle=True, # shuffle data after every epoch ?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is inside training Dataloader \n",
    "\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "# is used to extract a batch of data from a PyTorch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample\n",
    "torch.manual_seed(42)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item() \n",
    "                    # In PyTorch, the item() method is used to retrieve \n",
    "                    # the value of a tensor as a standard Python scalar. \n",
    "                    # It is typically used when you have a tensor with a single element, \n",
    "                    # such as a tensor representing a loss value or a single prediction.\n",
    "\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(\"Off\");\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a212779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfca3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatening the input\n",
    "\n",
    "# nn.Flatten() compresses the dimensions of a tensor into a single vector.\n",
    "\n",
    "flatten_model = nn.Flatten()    # all nn modules function as a model, \n",
    "                                # ie can do a forward pass \n",
    "# Getting a single sample \n",
    "\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Flattening the sample \n",
    "output = flatten_model(x) # forward pass \n",
    "\n",
    "# Printing output \n",
    "\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46508138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 0\n",
    "# Simple Linear Model \n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn.modules.linear import Linear\n",
    "\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Model \n",
    "\n",
    "model_0 = FashionMNISTModelV0(\n",
    "    input_shape=28*28,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names),\n",
    ").to(\"cpu\")\n",
    "\n",
    "print(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021eb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing from previous modules \n",
    "\n",
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup accuracy \n",
    "from helper_functions import accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy \n",
    "\n",
    "try:\n",
    "    from torchmetrics import Accuracy \n",
    "except:\n",
    "    !pip install torchmetrics\n",
    "    from torchmetrics import Accuracy\n",
    "\n",
    "accuracy = Accuracy(task=\"multiclass\",\n",
    "                    num_classes=len(class_names),\n",
    "                    )\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ded6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss() # This criterion computes the cross entropy loss\n",
    "                                # between input logits and target.\n",
    "\n",
    "print(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a16698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer \n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model_0.parameters(),\n",
    "    lr=0.1\n",
    ")\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48344e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for progress bar\n",
    "from tqdm.auto import trange, tqdm \n",
    "\n",
    "# Training Loop \n",
    "# Start seed \n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Number of Epochs \n",
    "epochs = 10\n",
    "\n",
    "# Training and testing loop\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(F\"Epoch: {epoch}\\n------\")\n",
    "\n",
    "    # Training \n",
    "    train_loss = 0\n",
    "\n",
    "    # loop to loop through the batches \n",
    "    for batch, (X,y) in tqdm(enumerate(train_dataloader)):    # enumerate() is a built-in function in Python \n",
    "                                                        # that allows you to iterate over a sequence \n",
    "                                                        # while keeping track of the index of each element. \n",
    "                                                        # It takes an iterable (such as a list, tuple, or string) \n",
    "                                                        # as input and returns an iterator that generates \n",
    "                                                        # pairs of index and corresponding elements.\n",
    "        model_0.train()\n",
    "\n",
    "        # 1. Forward Pass \n",
    "        y_pred = model_0(X) # output logits  \n",
    "\n",
    "        # 2. Calculate the loss \n",
    "        loss = loss_fn(y_pred, y) # loss for this specific batch\n",
    "                                  # input logits \n",
    "        train_loss += loss # accumulate loss per epoch \n",
    "\n",
    "        # 3. Optimizer Zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss baclward \n",
    "        loss.backward() # Backprop adone per batch \n",
    "\n",
    "        # 5.Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # print out how many batches sample have seen\n",
    "        if batch%400 == 0:\n",
    "            print(F\"Looked at {batch*len(X)}/{len(train_dataloader.dataset)} sampples\")\n",
    "\n",
    "    # Average loss per batch, per epoch \n",
    "    # Divide total train loss by length of train dataloader \n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Testing \n",
    "\n",
    "    test_loss, test_acc = 0,0\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X,y in tqdm(test_dataloader):\n",
    "            \n",
    "            # 1. Forward Pass\n",
    "            test_pred = model_0(X)\n",
    "\n",
    "            # 2. Calculate loss ( accumutively )\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "\n",
    "            # 3. Calculate Accuracy \n",
    "                                    # ( preds need to be same as y_true \n",
    "                                    # if using accuracy_fn from helper file\n",
    "                                    # if using torchmetrics accuracy, \n",
    "                                    # from docs : \n",
    "                                    # preds (Tensor): An int tensor of shape (N, ...) or \n",
    "                                    # float tensor of shape (N, C, ..). \n",
    "                                    # If preds is a floating point we apply torch.argmax \n",
    "                                    # along the C dimension to automatically convert probabilities/logits into an int tensor. )\n",
    "                                    # target (Tensor): An int tensor of shape (N, ...)\n",
    "\n",
    "            test_acc += accuracy(preds=test_pred, target=y )\n",
    "            #test_acc += accuracy(preds=test_pred.argmax(dim=1), target=y ) # this should work too \n",
    "            #test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1)) # jsut to test output accuracy format in %\n",
    "                                                                                # Found the bug\n",
    "                                                                                # if accuracy_fn be used, the accuracy is being multiplided by \n",
    "                                                                                # a factor of 100 in origical code implementation\n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of the test dataloader ( per batch )\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## print out whats happening \n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.4f}\\n\")\n",
    "    \n",
    "# Calculate Training time \n",
    "\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation\n",
    "torch.manual_seed(42)\n",
    "# make device agnostic\n",
    "\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy,\n",
    "               device: torch.device = device):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "        device (str, optional): Target device to compute on. Defaults to device.\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in data_loader: # using 32 images as a batch\n",
    "            \n",
    "            # Send data to the target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Making predictions\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            accuracy.to(device) \n",
    "            acc += accuracy(preds=y_pred, target=y)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader) \n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__ , # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\" : acc.item(), \n",
    "            } \n",
    "print(eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff50827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model 0 \n",
    "model_0_results = eval_model(model=model_0,\n",
    "                             data_loader=test_dataloader,\n",
    "                             loss_fn=loss_fn,\n",
    "                             accuracy=accuracy)\n",
    "print(model_0_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Linear model with ReLU \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Model \n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_1 = FashionMNISTModelV1(input_shape=28*28, # number of input features\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names) # number of output classes desired\n",
    ").to(device) # send model to GPU if it's available\n",
    "next(model_1.parameters()).device # check model device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701316c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy \n",
    "\n",
    "try:\n",
    "    from torchmetrics import Accuracy \n",
    "except:\n",
    "    !pip install torchmetrics\n",
    "    from torchmetrics import Accuracy\n",
    "\n",
    "accuracy = Accuracy(task=\"multiclass\",\n",
    "                    num_classes=len(class_names),\n",
    "                    )\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "# Loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "print(loss_fn)\n",
    "\n",
    "# Optimizer \n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3dc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Step\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy,\n",
    "               device: torch.device = device,\n",
    "               ):\n",
    "    \n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in tqdm(enumerate(data_loader)):\n",
    "        \n",
    "        # Sending data to GPU\n",
    "        X,y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward Pass\n",
    "        y_pred = model(X) # calculate logits \n",
    "\n",
    "        # 2. Calculate the loss \n",
    "        loss = loss_fn(y_pred, y) # loss for this specific batch\n",
    "                                  # input logits \n",
    "        train_loss += loss # accumulate loss per epoch\n",
    "        accuracy.to(device) \n",
    "        train_acc += accuracy(target=y, preds=y_pred)\n",
    "\n",
    "        # 3. Optimizer Zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss baclward \n",
    "        loss.backward() # Backprop adone per batch \n",
    "\n",
    "        # 5.Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Step \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy,\n",
    "              device: torch.device = device,\n",
    "              ):\n",
    "    \n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.eval() # put model in eval mode\n",
    "\n",
    "    \n",
    "    with torch.inference_mode(): # Turn on inference context manager\n",
    "        for X, y in tqdm(data_loader):\n",
    "\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            \n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            accuracy.to(device) \n",
    "            test_acc += accuracy(target=y, preds=test_pred)\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Measure time\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "train_time_start_on_gpu = timer()\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(F\"Epoch: {epoch} \\n------------\")\n",
    "    train_step(data_loader=train_dataloader,\n",
    "               model=model_1,\n",
    "               loss_fn=loss_fn,\n",
    "               optimizer=optimizer,\n",
    "               accuracy=accuracy,\n",
    "               )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=model_1,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy=accuracy,\n",
    "    )\n",
    "\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                            end=train_time_end_on_gpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a70e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_results = eval_model(model=model_1,\n",
    "                             data_loader=test_dataloader,\n",
    "                             loss_fn=loss_fn,\n",
    "                             accuracy=accuracy,\n",
    "                             device=device,)\n",
    "print(model_1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline results : \n",
    "print(model_0_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Convolutional Neural Network\n",
    "# Network Architecture from TinyVGG\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "\n",
    "    def __init__(self,input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1, #default\n",
    "                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "                                    )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units,hidden_units,3,1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3,1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "                                     )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7,\n",
    "                      out_features=output_shape)\n",
    "                                        )\n",
    "        \n",
    "    def forward(self,x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf331fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_2 = FashionMNISTModelV2(input_shape=1, \n",
    "    hidden_units=10, \n",
    "    output_shape=len(class_names)).to(device)\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbd53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create sample batch of random numbers with same size as image batch\n",
    "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
    "test_image = images[0] # get a single image for testing\n",
    "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\") \n",
    "print(f\"Single image pixel values:\\n{test_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbb432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy Conv2d\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a convolutional layer with same dimensions as TinyVGG \n",
    "# (try changing any of the parameters and see what happens)\n",
    "conv_layer = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=10,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=0)   # also try using \"valid\" or \"same\" here \n",
    "                                    # padding=0 and padding=\"valid\" both mean no padding is applied, \n",
    "                                    # while padding=\"same\" adds padding symmetrically \n",
    "                                    # to preserve the spatial dimensions of the input. \n",
    "\n",
    "                                    \n",
    "\n",
    "# Pass the data through the convolutional layer\n",
    "conv_layer(test_image)  # Note: If running PyTorch <1.11.0, \n",
    "                        # this will error because of shape issues \n",
    "                        #(nn.Conv.2d() expects a 4d tensor as input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra dimension to test image\n",
    "test_image.unsqueeze(dim=0).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ee95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass test image with extra dimension through conv_layer\n",
    "conv_layer(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Create a new conv_layer with different values (try setting these to whatever you like)\n",
    "conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n",
    "                         out_channels=10,\n",
    "                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n",
    "                         stride=2,\n",
    "                         padding=0)\n",
    "\n",
    "# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\n",
    "conv_layer_2(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d364440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the conv_layer_2 internal parameters\n",
    "print(conv_layer_2.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526aefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shapes of weight and bias tensors within conv_layer_2\n",
    "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n",
    "print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels=10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out original image shape without and with unsqueezed dimension\n",
    "print(f\"Test image original shape: {test_image.shape}\")\n",
    "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
    "\n",
    "# Create a sample nn.MaxPoo2d() layer\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "# Pass data through just the conv_layer\n",
    "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
    "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
    "\n",
    "# Pass data through the max pool layer\n",
    "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
    "print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Create a random tensor with a similiar number of dimensions to our images\n",
    "random_tensor = torch.randn(size=(1, 1, 2, 2))\n",
    "print(f\"Random tensor:\\n{random_tensor}\")\n",
    "print(f\"Random tensor shape: {random_tensor.shape}\")\n",
    "\n",
    "# Create a max pool layer\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n",
    "\n",
    "# Pass the random tensor through the max pool layer\n",
    "max_pool_tensor = max_pool_layer(random_tensor)\n",
    "print(f\"\\nMax pool tensor:\\n{max_pool_tensor} <- this is the maximum value from random_tensor\")\n",
    "print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177aac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), \n",
    "                             lr=0.1)\n",
    "try:\n",
    "    from torchmetrics import Accuracy \n",
    "except:\n",
    "    !pip install torchmetrics\n",
    "    from torchmetrics import Accuracy\n",
    "\n",
    "accuracy = Accuracy(task=\"multiclass\",\n",
    "                    num_classes=len(class_names),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_model_2 = timer()\n",
    "\n",
    "# Train and test model \n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model_2, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy=accuracy,\n",
    "        device=device\n",
    "    )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=model_2,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy=accuracy,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
    "                                           end=train_time_end_model_2,\n",
    "                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model_2 results \n",
    "model_2_results = eval_model(\n",
    "    model=model_2,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy=accuracy\n",
    ")\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f80380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\n",
    "compare_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add training times to results comparison\n",
    "compare_results[\"training_time\"] = [total_train_time_model_0,\n",
    "                                    total_train_time_model_1,\n",
    "                                    total_train_time_model_2]\n",
    "compare_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our model results\n",
    "compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\n",
    "plt.xlabel(\"accuracy (%)\")\n",
    "plt.ylabel(\"model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da07486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
    "\n",
    "    pred_probs = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for sample in data:\n",
    "\n",
    "            #Prepare sample \n",
    "            sample = torch.unsqueeze(sample, dim=0).to(device)\n",
    "\n",
    "            # Forward Pass, get raw logit \n",
    "            pred_logit = model(sample)\n",
    "\n",
    "            # Prediction probability \n",
    "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
    "\n",
    "            # Get pred_prob off GPU for further calculations\n",
    "            pred_probs.append(pred_prob.cpu())\n",
    "    \n",
    "    # Stack the pred_probs to turn list into a tensor\n",
    "    return torch.stack(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63255658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "for sample, label in random.sample(list(test_data), k=9):\n",
    "    test_samples.append(sample)\n",
    "    test_labels.append(label)\n",
    "\n",
    "# View the first test sample shape and label\n",
    "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test samples with model 2\n",
    "pred_probs= make_predictions(model=model_2, \n",
    "                             data=test_samples)\n",
    "\n",
    "# View first two prediction probabilities list\n",
    "print(torch.round(pred_probs[:2], decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
    "pred_classes = pred_probs.argmax(dim=1)\n",
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e836d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are our predictions in the same form as our test labels? \n",
    "test_labels, pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.eq(torch.Tensor(test_labels), pred_classes)\n",
    "print(a)\n",
    "print(a.sum()/len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.figure(figsize=(9, 9))\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "for i, sample in enumerate(test_samples):\n",
    "  # Create a subplot\n",
    "  plt.subplot(nrows, ncols, i+1)\n",
    "\n",
    "  # Plot the target image\n",
    "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
    "\n",
    "  # Find the prediction label (in text form, e.g. \"Sandal\")\n",
    "  pred_label = class_names[pred_classes[i]]\n",
    "\n",
    "  # Get the truth label (in text form, e.g. \"T-shirt\")\n",
    "  truth_label = class_names[test_labels[i]] \n",
    "\n",
    "  # Create the title text of the plot\n",
    "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
    "  \n",
    "  # Check for equality and change title colour accordingly\n",
    "  if pred_label == truth_label:\n",
    "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
    "  else:\n",
    "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
    "  plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Make predictions with trained model\n",
    "y_preds = []\n",
    "model_2.eval()\n",
    "with torch.inference_mode():\n",
    "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
    "    # Send data and targets to target device\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    # Do the forward pass\n",
    "    y_logit = model_2(X)\n",
    "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
    "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n",
    "    # Put predictions on CPU for evaluation\n",
    "    y_preds.append(y_pred.cpu())\n",
    "# Concatenate list of predictions into a tensor\n",
    "y_pred_tensor = torch.cat(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if torchmetrics exists, if not, install it\n",
    "try:\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
    "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
    "except:\n",
    "    !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab, this may require restarting the runtime\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlxtend version: {mlxtend.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f92c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix \n",
    "\n",
    "#  Setup confusion matrix instance and compare predictions to targets\n",
    "\n",
    "confmat = ConfusionMatrix(num_classes=len(class_names), \n",
    "                          task=\"multiclass\")\n",
    "\n",
    "confmat_tensor = confmat(preds=y_pred_tensor,\n",
    "                         target=test_data.targets)\n",
    "\n",
    "# Plotting \n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n",
    "    class_names=class_names, # turn the row and column labels into class names\n",
    "    figsize=(10, 7)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde86cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(test_data.targets, y_pred_tensor, display_labels=class_names,xticks_rotation=\"vertical\")\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d845c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized \n",
    "ConfusionMatrixDisplay.from_predictions(test_data.targets, \n",
    "                                        y_pred_tensor, \n",
    "                                        display_labels=class_names,\n",
    "                                        normalize='true',\n",
    "                                        xticks_rotation=\"vertical\",\n",
    "                                        values_format=\".0%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = (test_data.targets != y_pred_tensor)\n",
    "ConfusionMatrixDisplay.from_predictions(test_data.targets, \n",
    "                                        y_pred_tensor,\n",
    "                                        sample_weight=sample_weight, \n",
    "                                        display_labels=class_names,\n",
    "                                        normalize='true',\n",
    "                                        xticks_rotation=\"vertical\",\n",
    "                                        values_format=\".0%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717df77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = (test_data.targets != y_pred_tensor)\n",
    "ConfusionMatrixDisplay.from_predictions(test_data.targets, \n",
    "                                        y_pred_tensor,\n",
    "                                        sample_weight=sample_weight, \n",
    "                                        display_labels=class_names,\n",
    "                                        normalize='pred',\n",
    "                                        xticks_rotation=\"vertical\",\n",
    "                                        values_format=\".0%\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd7a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
